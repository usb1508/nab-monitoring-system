# NAB Anomaly Detection + Monitoring System

This project implements a hybrid anomaly detection model based on the Numenta Anomaly Benchmark (NAB) framework, along with a Prometheus + Grafana-based monitoring and alerting system.

---

## Project Setup

### 1. Clone this repository

```bash
git clone https://github.com/usb1509/nab-monitoring-system.git
cd nab-monitoring-system
```

### 2. Clone NAB repository inside this project

```bash
git clone https://github.com/numenta/NAB.git
```

> **Note:** The NAB folder should be at the root level of the project (sibling to `monitor/`).

---

## Anomaly Detection

### 3. Install Python dependencies

```bash
pip install -r requirements.txt
```

### 4. Run the hybrid anomaly detector

```bash
python src/my_hybrid_model.py
```

This step processes the NAB data and generates anomaly scores under the `NAB/results/` directory.

> You can also copy the results directly from the NAB repository, but note that these will not include results generated by our custom model.

---

### 5. Benchmark using NAB scripts

Navigate to the `NAB/` folder:

```bash
cd NAB
```

Create a new detector entry:

```bash
python -m scripts.create_new_detector --detector my_model
```

Run the NAB scoring and optimization:

```bash
python run.py -d my_model_1 --optimize --score --normalize
```

This will evaluate the detector and save scoring results.

---

## Monitoring Setup

### 6. Prepare monitoring environment

After generating the results, **copy the `results/` folder** you created into the `monitor/` directory.

The structure should be:

```
/nab-monitoring-system/
    /monitor/
        /results/   <-- copied here
```

---

### 7. Start the monitoring stack (Prometheus + Grafana)

Navigate to the `monitor/` folder:

```bash
cd ../monitor
```

Then run:

```bash
docker compose up
```

This will spin up the monitoring system automatically.

---

### 8. (Optional) Set up Alerting via Email

If you want to enable email alerts, create a `.env` file inside the `monitor/` directory with the following contents:

```env
GMAIL_USER=<YOUR_GMAIL_USERNAME>
GMAIL_PASSWORD=<YOUR_APP_PASSWORD>
```

- If you **do not** create the `.env` file or leave dummy values, the alert system will simply not trigger email notifications.

> **Note:** Alerts, Prometheus exporters, Grafana dashboards, and data sources are all created programmatically — no manual UI configuration is required.

---

## What We Are Monitoring

The system includes a custom **Prometheus exporter** that reads the anomaly scores generated by our detector and streams them as metrics.

- Each anomaly score is exposed at `/metrics` using FastAPI and `prometheus_client`.
- Metrics are labeled with:
  - `model` (the model name)
  - `dataset` (dataset folder)
  - `file` (CSV filename)

Example metric:
```text
nab_anomaly_score{model="my_model_1",dataset="realKnownCause",file="machine_temperature.csv"} 0.72
```

> The anomaly scores are continuously streamed from result CSVs at a configurable rate (default: 30 rows/sec).

Prometheus scrapes this endpoint, and Grafana dashboards visualize the scores in real-time.  
No manual UI configuration is needed — dashboards, data sources, and alerts are all **created automatically using code**.

---

## Testing the Monitoring System

A simple **test server** is also provided to simulate metrics:

- File: `monitor/fake_prometheus_server.py`
- It exposes random anomaly scores at `localhost:8001`.

To run the test server manually (if needed):

```bash
python monitor/fake_prometheus_server.py
```

This is **optional** — by default, the system monitors the actual exporter, and alerts if the real exporter is down.

---

## About the Anomaly Detection Algorithm

This project implements a **hybrid anomaly detection model** combining several techniques:

- **Isolation Forest** for unsupervised anomaly detection
- **STL decomposition** to isolate seasonal components
- **Change point detection** using `ruptures`
- **Rolling statistics** (mean, std, z-score)

The final anomaly score is an **ensemble** of:
- Isolation Forest prediction
- Z-score outliers
- Changepoint detection signals

This hybrid approach allows detecting both point anomalies and structural shifts in time series data.

---

---
---

## Benchmark Scores on NAB Dataset

The hybrid anomaly detection model was benchmarked using the official NAB evaluation framework.

Here are the final scores achieved:

| Profile                  | Final Score |
|:--------------------------|------------:|
| Standard Profile          | **51.43**   |
| Reward Low False Positives| **41.44**   |
| Reward Low False Negatives| **63.89**   |

---

## Comparison with NAB Leaderboard

For reference, scores from well-known detectors on the NAB public scoreboard are:

| Detector                  | Standard | Low FP  | Low FN |
|:---------------------------|---------:|--------:|-------:|
| Perfect Detector           | 100.0    | 100.0   | 100.0  |
| ARTime                     | 74.9     | 65.1    | 80.4   |
| Numenta HTM                | 70.5–69.7| 62.6–61.7| 75.2–74.2 |
| Random Cut Forest          | 51.7     | 38.4    | 59.7   |
| **Our Hybrid Detector**    | **51.43** | **41.44** | **63.89** |
| Twitter ADVec              | 47.1     | 33.6    | 53.5   |
| Windowed Gaussian          | 39.6     | 20.9    | 47.4   |
| Bayesian Changepoint       | 17.7     | 3.2     | 32.2   |
| Null Detector              | 0.0      | 0.0     | 0.0    |

---

 **Interpretation**:
- Perfect score is **100** on all profiles.
- Most practical detectors (like Random Cut Forest, Twitter ADVec) score in the **40–70 range**.
- Our model scores **51.43** on Standard Profile — putting it **within the competitive range** compared to many real-world anomaly detection systems.

All final scores are written to:

```
NAB/results/final_results.json
```

> **Note:** Scoring and normalization are handled automatically using NAB’s `run.py` with `--optimize --score --normalize` flags.

---



## Requirements

- Python 3.8+
- Docker and Docker Compose (for monitoring stack)

---

## License

This project is licensed under the MIT License.
